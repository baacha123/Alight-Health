# =============================================================================
# GovCloud Evaluation Configuration
# =============================================================================
# This config is used by govcloud_eval.py to run evaluations on GovCloud/FedRAMP.
#
# SETUP:
#   1. Run `orchestrate models list` to see available models
#   2. Update model IDs if needed (defaults work for most GovCloud instances)
#   3. Update paths.test_cases to point to your ground truth folder
#
# =============================================================================

# === Models ===
# These are GovCloud-available models (405b is NOT available on GovCloud)
models:
  llm_user: "meta-llama/llama-3-2-90b-vision-instruct"
  llm_judge: "meta-llama/llama-3-2-90b-vision-instruct"
  embedding: "sentence-transformers/all-minilm-l6-v2"

# === Paths ===
paths:
  # Directory containing your ground truth test case JSON files
  test_cases: "./test_data"

  # Directory where evaluation results will be saved
  output_dir: "./eval_results_govcloud"

# === Provider Settings ===
provider:
  type: "gateway"
  vendor: "ibm"

# === Evaluation Settings ===
evaluation:
  num_workers: 1
  n_runs: 1
  similarity_threshold: 0.8
  enable_fuzzy_matching: false
  is_strict: true
